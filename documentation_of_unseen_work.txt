


Overview of issues related to tweet data

In all, it took more than 2 weeks of crutial time (after the relevant code had been written) to download, process, filter, and merge the data. The json unzipping process had to be stopped since the data would have exceeded their largest disk size (4 TB). Only a portion of the unzipped files were able to be concatinated together (~550 GB) because of disk contraints. Then, this data was heavily filtered down to the relevant slice of the tweets data. The result is a small dataset of tweet jsons (150 MB) that we based our analysis off of, however, this was due more to archive.org and VM limitations than anything we had control over.


Tweets Download Speed

The servers that we are downloading twitter data from we very slow due to things out of our control. Runing the bash script to get these files required using the screen command and 30+ hours each time that the data needed to be downloaded (this was repeated twice). Then these tar files had to be compiled into a single omnibus file through another bash script (at which point the file was added to a google cloud bucket). 

The process had to be repeated twice since unzipping the files increased the data's harddrive footprint more than expected, thus a 4 TB VM had to be rented to fit the data during all of the intermediary steps.

Opening these tar files and then unzipping each of the json files within them took multiple days because of the size of the dataset.


Weather Data Misformating

Data from the NOAA was very poorly formatted and required the dataset to be broken up and remerged in order for the data to be meaningful. sqlite3 could not handle a dataset of this size so another database library had to be used, but mysql was also not ideally situated to deal with a dataset of this size (it caused the VM to continually freeze).

